# Sign-language-recognition-glove-enhanced-by-visual-sensory-based-multimodality-machine-learning
Final Year Project at National University of Singapore (Suzhou) Research Institute.
We aim to integrate the methods of computer vision and sensors to create an AI-based system that can help recognize certain gestures in the American Sign Langauge, so as to help the speech & hearing impaired people better communicate.
This project is still ongoing.

Please do not use the code at present, I'm still improving, thanks!

Because of the github upload file size limit, I put the images of the three data sets into the Baidu web disk：
1. the first dataset (only visual)
link：https://pan.baidu.com/s/1XmfqhW932LrJucSh1d5iOA 
passwd：x1y3
2. the second dataset (visual+somatosensory, the somatosensory part is in 'second_dataset_sensor_data.xlsx')
link：https://pan.baidu.com/s/1IG_Ga3cGqLsZUeFtokzYsA 
passwd：b68c
3. the third dataset (visual+somatosensory, the somatosensory part is in 'third_dataset_sensor_data.xlsx')
link：https://pan.baidu.com/s/1J33hgJg9LK6cFgRJkLINqg 
passwd：qtqp
