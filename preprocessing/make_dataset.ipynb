{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaac89f4-ec47-42dd-bf6f-5bf9914813d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pdb\n",
    "\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split, Dataset, TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86cafee2-ec70-420f-b0cf-32759207b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'data': self.data[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbbab49-3ac6-4903-a374-7f15a014d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "img_data = torch.load('/mnt/fyp/data/img_data.pt').reshape(3995,-1)\n",
    "teng_data = torch.load('/mnt/fyp/data/teng_data.pt')\n",
    "imu_data = torch.load('/mnt/fyp/data/imu_data.pt')\n",
    "# img_data: 3995*(3*224*224)\n",
    "# teng_data: 3995*(50*10)\n",
    "# imu_data: 3995*(50*18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a176cf68-947f-4d4a-be62-1daab966fd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3995, 151928])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge data\n",
    "merged_data = torch.cat((img_data, teng_data, imu_data),dim=1)\n",
    "# evaluate\n",
    "merged_data.shape # must be 3995*151928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7455be-db15-43bb-828d-30010ecea4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3893, 151928])\n",
      "torch.Size([3790, 151928])\n",
      "torch.Size([3689, 151928])\n",
      "torch.Size([3588, 151928])\n",
      "torch.Size([3486, 151928])\n",
      "torch.Size([3382, 151928])\n",
      "torch.Size([3278, 151928])\n",
      "torch.Size([3171, 151928])\n",
      "torch.Size([3069, 151928])\n",
      "torch.Size([2968, 151928])\n",
      "torch.Size([2866, 151928])\n",
      "torch.Size([2764, 151928])\n",
      "torch.Size([2657, 151928])\n",
      "torch.Size([2556, 151928])\n",
      "torch.Size([2452, 151928])\n",
      "torch.Size([2351, 151928])\n",
      "torch.Size([2250, 151928])\n",
      "torch.Size([2148, 151928])\n",
      "torch.Size([2046, 151928])\n",
      "torch.Size([1945, 151928])\n",
      "torch.Size([1844, 151928])\n",
      "torch.Size([1742, 151928])\n",
      "torch.Size([1640, 151928])\n",
      "torch.Size([1538, 151928])\n",
      "torch.Size([1437, 151928])\n",
      "torch.Size([1336, 151928])\n",
      "torch.Size([1228, 151928])\n",
      "torch.Size([1126, 151928])\n",
      "torch.Size([1025, 151928])\n",
      "torch.Size([923, 151928])\n",
      "torch.Size([822, 151928])\n",
      "torch.Size([719, 151928])\n",
      "torch.Size([618, 151928])\n",
      "torch.Size([517, 151928])\n",
      "torch.Size([413, 151928])\n",
      "torch.Size([312, 151928])\n",
      "torch.Size([205, 151928])\n",
      "torch.Size([104, 151928])\n",
      "torch.Size([0, 151928])\n"
     ]
    }
   ],
   "source": [
    "count = torch.load('/mnt/fyp/data/count.pt')\n",
    "\n",
    "for i,num in enumerate(count):\n",
    "    if i==0:\n",
    "        selected_merged_data = merged_data[:min(num,100), :]\n",
    "    else:\n",
    "        selected_merged_data = torch.cat((selected_merged_data, merged_data[:min(num,100), :]), dim=0)\n",
    "        \n",
    "    # split\n",
    "    merged_data = merged_data[max(num, 100):, :]\n",
    "    print(merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d759e3a1-2809-4b70-bc89-2406b1a88fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3900, 151928])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c98750c6-c2ce-4877-87e4-7d181d01d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "class_data = [[] for _ in range(39)]\n",
    "selected_labels = torch.tensor([i for i in range(39) for _ in range(100)]).reshape(-1,1)\n",
    "\n",
    "for class_idx in range(39):\n",
    "    class_mask = (selected_labels == class_idx).squeeze()\n",
    "    data = selected_merged_data[class_mask]\n",
    "    labels = selected_labels[class_mask]\n",
    "    \n",
    "    train_size = int(0.8 * len(data)) # must be 80\n",
    "    val_size = len(data) - train_size # must be 20\n",
    "    \n",
    "    # random indices\n",
    "    num_samples = len(data) # must be 100\n",
    "    random_indices = torch.randperm(num_samples)\n",
    "    \n",
    "    train_data = data[random_indices[:train_size]]\n",
    "\n",
    "    val_data = data[random_indices[train_size:]]\n",
    "    \n",
    "    class_data[class_idx] = (train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835e4cf3-a5bb-42ae-b5ab-149d99090820",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(39):\n",
    "    if i==0:\n",
    "        train_data = class_data[i][0]\n",
    "        val_data = class_data[i][1]\n",
    "    else:\n",
    "        train_data = torch.cat((train_data, class_data[i][0]),dim=0)\n",
    "        val_data = torch.cat((val_data, class_data[i][1]),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7b0d11-95e5-4954-b4c3-02104edaa5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = [i for i in range(39) for _ in range(80)]\n",
    "val_label = [i for i in range(39) for _ in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b4da50-10fd-4d67-b6c1-02199cb2e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GestureDataset(train_data, train_label)\n",
    "val_dataset = GestureDataset(val_data, val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb09ccb-9690-42a6-bbee-ce0c59751f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # enable shuffle and drop_last\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53a7252d-4567-4ef0-abc4-9ec13e347760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 151928])\n",
      "torch.Size([64])\n",
      "tensor([ 5, 18, 11, 14, 24, 38, 22, 24, 23, 31, 10, 31, 16, 26,  1, 25,  4, 26,\n",
      "        38, 14, 25, 33, 18, 28, 30, 36,  1, 25,  8, 35, 19, 11, 22, 14, 26, 30,\n",
      "        31, 19,  6, 17, 23, 34,  6,  5,  9, 31, 18,  1,  8, 35,  5, 28, 26, 13,\n",
      "        17,  0, 36,  8, 28, 24, 25, 37, 23, 24])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.load('/mnt/fyp/data/train_loader')\n",
    "for batch in train_loader:\n",
    "    print(batch['data'].shape)\n",
    "    print(batch['label'].shape)\n",
    "    print(batch['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372413c9-365a-4110-b9ce-dd15dd790cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
